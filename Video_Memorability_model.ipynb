{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySsZRDpWM9c0"
      },
      "source": [
        "<h1><center>Final Year Project 2023 <br> Video Summarization using Memorability Scores</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKqjCOUzt7T5"
      },
      "source": [
        "<h1><center>\n",
        "Video Memorability Prediction<br>\n",
        "Ramya priya S<br>\n",
        "Thushita Mariaselvan\n",
        "</center></h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxnZ7zUE_I37"
      },
      "source": [
        "## Google Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSf-MKob36_p",
        "outputId": "458eb6fc-0996-4508-fd71-1d4261848a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZvoy7Lb_Miy"
      },
      "source": [
        "## Import and Installing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmkA_Bi4WR4m"
      },
      "source": [
        "Few main pakages used in this project are as follows:\n",
        "\n",
        "> * **pyprind:** The PyPrind (Python Progress Indicator) module provides a progress bar and a percentage indicator object that let you track the progress of a loop structure or other iterative computation.\n",
        "> * **Keras:** is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow. Designed to enable fast experimentation with deep neural networks.\n",
        "> * **TensorFlow:** is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks.\n",
        "> * **Scikit-learn:** is a machine learning library for the Python. It features various classification, regression and clustering algorithms including support vector machines, random forests.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pDL8mpgu7KP",
        "outputId": "e07434e3-32d9-4a51-cd8b-ed797c32c649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.0.4)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyprind\n",
        "!pip install -U keras\n",
        "!pip install keras\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmFzFFX5vG0-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import keras\n",
        "from keras import Sequential, layers, regularizers, optimizers\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "import pyprind\n",
        "from collections import Counter\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from natsort import natsorted,ns\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IJx21Pp_cIE"
      },
      "source": [
        "## Setting seed for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMBK6SucZVBI"
      },
      "source": [
        "By doing so our program generates the same set of random inputs to help us reproduce the results later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLxeWLzD_bw_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeR8VweX_p5X"
      },
      "source": [
        "## Spearman's Correlation Function To Calculate Memorability Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2_qyjLKZnOv"
      },
      "source": [
        ">  **Spearman's rank correlation coefficient :** is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a linear function. We have calculated the spearman score between the predicted and actual values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWkpm_TB_jpl"
      },
      "outputs": [],
      "source": [
        "def Get_score(Y_pred,Y_true):\n",
        "    '''Calculate the Spearmann\"s correlation coefficient'''\n",
        "    Y_pred = np.squeeze(Y_pred)\n",
        "    Y_true = np.squeeze(Y_true)\n",
        "    if Y_pred.shape != Y_true.shape:\n",
        "        print('Input shapes don\\'t match!')\n",
        "    else:\n",
        "        #if it has only long / short term calc\n",
        "        if len(Y_pred.shape) == 1:\n",
        "            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})\n",
        "            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)\n",
        "            print('The Spearman\\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])\n",
        "        else:\n",
        "          #if it has both long ans short term calc for both seperately and print\n",
        "            for ii in range(Y_pred.shape[1]):\n",
        "                Get_score(Y_pred[:,ii],Y_true[:,ii])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKIRFKq5_289"
      },
      "source": [
        "## Features\n",
        "\n",
        "---\n",
        "We are going to load 3 features from the dataset.\n",
        "\n",
        "*   Convolution 3D (C3D) -- Generic features generated for video analysis.\n",
        "> C3D is obtained by training a deep 3D convolutional network on a large annotated video dataset. The dataset contains various concepts encompassing objects, actions, scenes and other frequently occurring categories in videos.\n",
        "\n",
        "*   Histogram of Motion Patterens (HMP) -- Histogram of motion patterns generated for each video\n",
        "> HMP is a static image template which helps in understanding the motion location and path as it progresses. The temporal motion information is collapsed into a single image template where intensity is a function of recency of motion, where brighter values correspond to a more recent motion.\n",
        "\n",
        "*  Captions -- Textual features describing the videos.\n",
        "> Captions provide a textual description of an video based on the objects and actions in it.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdzB3NYa_58k"
      },
      "source": [
        "### Functions for Loading different Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjNfb5AUvWgm"
      },
      "outputs": [],
      "source": [
        "# captions\n",
        "def read_caps(fname):\n",
        "    \"\"\"Load the captions into a dataframe\"\"\"\n",
        "    vn = []\n",
        "    cap = []\n",
        "    df = pd.DataFrame();\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            pairs = line.split()\n",
        "            vn.append(pairs[0])\n",
        "            cap.append(pairs[1])\n",
        "        df['video']=vn\n",
        "        df['caption']=cap\n",
        "    return df\n",
        " \n",
        "# loading C3D\n",
        "def read_C3D(fname):\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            C3D =[float(item) for item in line.split()] \n",
        "    return C3D\n",
        "  \n",
        "# loading HMP\n",
        "def read_HMP(fname):\n",
        "    \"\"\"Scan HMP(Histogram of Motion Patterns) features from file\"\"\"\n",
        "    #It is in form of bin number : values\n",
        "    #Hence getting these and storing it in dict in {bin no:value} form \n",
        "    #then storing it in list with index as bin no\n",
        "    with open(fname) as f:\n",
        "        for line in f:\n",
        "            pairs=line.split()\n",
        "            HMP_temp = { int(p.split(':')[0]) : float(p.split(':')[1]) for p in pairs}\n",
        "    # there are 6075 bins, fill zeros\n",
        "    HMP = np.zeros(6075)\n",
        "    for idx in HMP_temp.keys():\n",
        "        HMP[idx-1] = HMP_temp[idx]            \n",
        "    return HMP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq3OihWjAPZL"
      },
      "source": [
        "## Declaring Source Paths And Variables\n",
        "> Ground Truth values are loaded as labels and contains:\n",
        "*   Video's name\n",
        "*   Short-term memorability score.\n",
        "*   Number of annotations which was used to calculate its short-term memorability score.\n",
        "*   Long-term memorability score.\n",
        "*   Number of annotations which was used to calculate its long-term memorability score.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdB9X-RGvbnf"
      },
      "outputs": [],
      "source": [
        "# load the captions\n",
        "cap_path = '/content/drive/MyDrive/dev-set_video-captions.txt'\n",
        "df_cap=read_caps(cap_path)\n",
        "\n",
        "# load the ground truth values\n",
        "label_path = '/content/drive/MyDrive/ground-truth_dev-set.csv'\n",
        "labels=pd.read_csv(label_path)\n",
        "\n",
        "# Path for the features\n",
        "Feat_path = '/content/drive/MyDrive/'\n",
        "\n",
        "#getting video names using features' name\n",
        "def getnames(featurename):\n",
        "  vnames = os.listdir(Feat_path+featurename)\n",
        "  return vnames\n",
        "\n",
        "#get full path of the features using features' name \n",
        "def getpaths(featurename):\n",
        "  fpath = [Feat_path+featurename+'/' + x for x in  getnames(featurename)]\n",
        "  sorted = natsorted(fpath,alg=ns.IGNORECASE)\n",
        "  print('Feature Path:')\n",
        "  print(sorted[:5])\n",
        "  return sorted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bBgexmSAVlS"
      },
      "source": [
        "### Functions Definitions\n",
        "> For reading HMP and C3D text files into an numpy array\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctmmIAWYvfkp"
      },
      "outputs": [],
      "source": [
        "#load C3D and return as an array of list\n",
        "def df_C3D():\n",
        "  path = getpaths('C3D')\n",
        "  c3d = []\n",
        "  print('Loading C3D')\n",
        "  for item in path:\n",
        "    c3d.append(read_C3D(item))\n",
        "  print('done')\n",
        "  return np.asarray(c3d)\n",
        "\n",
        "#load HMP and return as an array of list\n",
        "def df_HMP():\n",
        "  path = getpaths('HMP')\n",
        "  hmp = []\n",
        "  print('Loading HMP')\n",
        "  for item in path:\n",
        "    if 'video' not in item:\n",
        "      continue\n",
        "    hmp.append(read_HMP(item))\n",
        "  print('done')\n",
        "  return np.asarray(hmp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QMDenysAlJ5"
      },
      "source": [
        "### Loading C3D and HMP features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n51qwqAevjm9",
        "outputId": "21b4e188-7cc8-4703-a236-e28013532283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Path:\n",
            "['/content/drive/MyDrive/C3D/0n3gVXmbG-I_part_0.txt', '/content/drive/MyDrive/C3D/0n3gVXmbG-I_part_1.txt', '/content/drive/MyDrive/C3D/0n3gVXmbG-I_part_2.txt', '/content/drive/MyDrive/C3D/0n3gVXmbG-I_part_3.txt', '/content/drive/MyDrive/C3D/3-sFLOcz_1I_part_0.txt']\n",
            "Loading C3D\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "#loading C3D\n",
        "feature1 = df_C3D()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq9wMraHvroE",
        "outputId": "15194849-02ab-4243-a61f-da192d1abba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Path:\n",
            "['/content/drive/MyDrive/HMP/0n3gVXmbG-I_part_0.txt', '/content/drive/MyDrive/HMP/0n3gVXmbG-I_part_1.txt', '/content/drive/MyDrive/HMP/0n3gVXmbG-I_part_2.txt', '/content/drive/MyDrive/HMP/0n3gVXmbG-I_part_3.txt', '/content/drive/MyDrive/HMP/0n3gVXmbG-I_part_4.txt']\n",
            "Loading HMP\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "feature2 = df_HMP()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REwDjltwnwnl"
      },
      "source": [
        "## Cleaning / Preprocessing of features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfZCVlIVA5gc"
      },
      "source": [
        "### Optimizing captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW_JqFaoayVm"
      },
      "source": [
        "Natural language toolkit library has be used to optimize the captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADoE07TkwR1z"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_NNl6GIZSxF"
      },
      "source": [
        "> Stripping Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Y96Rz6-n_6"
      },
      "outputs": [],
      "source": [
        "df = df_cap.copy()\n",
        "import re\n",
        "def strip_character(dataCol):\n",
        "    r = re.compile(r'[^a-zA-Z]')\n",
        "    #substituting special characters with space\n",
        "    return r.sub(' ', str(dataCol))\n",
        "\n",
        "df['caption'] = df['caption'].apply(strip_character)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRLTogY4-8-n",
        "outputId": "15efa957-2069-48de-e593-c71910073c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDu46r_-Z83a"
      },
      "source": [
        "\n",
        "\n",
        "> Removing Stopwords\n",
        "######Stop word removal is one of the most commonly used preprocessing steps across different NLP applications. The idea is simply removing the words that occur commonly across all the documents in the corpus. Typically, articles and pronouns are generally classified as stop words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esZXTxGTBHUD"
      },
      "outputs": [],
      "source": [
        "stop = stopwords.words('english') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Raax_MF0BJoC",
        "outputId": "c01b8c48-de23-4e7a-d35b-c9432cac338e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            couple relaxing picnic crane shot\n",
              "1    cute black white cats cage animal shelter\n",
              "2                               owl tree close\n",
              "3                          around house laptop\n",
              "4                   beautiful young girl apple\n",
              "Name: caption, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df['caption'] = df['caption'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "df['caption'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdwfbSEaaWGi"
      },
      "source": [
        "\n",
        "\n",
        "> Lemmatization\n",
        "*   Lemmatisation is a process of grouping words together which inflict same forms of a word so they can be analysed as a single item.\n",
        "* For example, reducing \"builds\", \"building\", or \"built\" to the lemma \"build\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWIPRNVSBLl0",
        "outputId": "09ef4a3c-ecd7-4026-f3ea-254ab74b0249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            couple relaxing picnic crane shot\n",
              "1    cute black white cats cage animal shelter\n",
              "2                               owl tree close\n",
              "3                          around house laptop\n",
              "4                   beautiful young girl apple\n",
              "Name: caption, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "df['caption'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]',' ',text)) for text in lis]) for lis in df['caption']]\n",
        "df['caption'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUb55NTQbGLF"
      },
      "source": [
        "\n",
        "\n",
        "> Extracting Unique Word Count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbzrWFfFBNth"
      },
      "outputs": [],
      "source": [
        "counts = Counter()\n",
        "# pbar = pyprind.ProgBar(len(df['caption']), title='Counting word occurrences')\n",
        "for i, cap in enumerate(df['caption']):\n",
        "    counts.update(cap.split())   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMg3gq1-BQoE",
        "outputId": "34ebc3b5-8d02-4b6d-d63e-e146ab240a26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['couple relaxing picnic crane shot',\n",
              "       'cute black white cats cage animal shelter', 'owl tree close', ...,\n",
              "       'tombstone hattie wife private charles thompson',\n",
              "       'closeup green vegetation desert field',\n",
              "       'fisherman step shore corpus christi texas'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "df.caption.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV1n8crqbOWO"
      },
      "source": [
        "\n",
        "\n",
        "> n-Gram approach using TF - IDF\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfYvVzRGBS09"
      },
      "outputs": [],
      "source": [
        "vect = TfidfVectorizer(ngram_range = (1,4)).fit(df.caption)\n",
        "vect_transformed_X_train = vect.transform(df.caption)\n",
        "len_token = len(vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKCTLk0rBU1f",
        "outputId": "f402dee8-c1ab-4159-ca4e-6ad7a17725b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74842"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLf3dvfecd66"
      },
      "source": [
        "\n",
        "\n",
        ">  Maping each unique word in the vector to an integer (one-hot encoding)\n",
        "\n",
        "\n",
        "*   Building word index by using tokenizer\n",
        "*   Fitting a list of captions to the tokinizer\n",
        "> tokenizer vectorizes a text corpus, by turning each text into a sequence of integers\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP6EarfjBWvo"
      },
      "outputs": [],
      "source": [
        "len_token = len(counts)\n",
        "tokenizer = Tokenizer(num_words=len_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm2iaBz8BYzQ"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(list(vect.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmZJ93RvBasw"
      },
      "outputs": [],
      "source": [
        "captions = tokenizer.texts_to_matrix(list(df.caption.values),mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIXd35nDBcj2",
        "outputId": "06c0ba38-a8f1-434f-e066-803c5b6e0d05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "len(captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0zShc3hBiYC",
        "outputId": "c6815107-0f3d-4bfb-ebc2-19805963b6c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000 8000\n"
          ]
        }
      ],
      "source": [
        "#Checking length. Output should be (8000,8000)\n",
        "feature1 = feature1[:8000]\n",
        "print(len(feature1),len(feature2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hylfIxjHaGED"
      },
      "source": [
        "###Testing data captions preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weDW_VfFaL58",
        "outputId": "e5db8fcc-2c53-402b-ec71-006e5148faf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    young girl sitting table talks plate food\n",
            "1             children look bag take something\n",
            "2     woman comes bathroom downstairs laughing\n",
            "3                    women driving car talking\n",
            "4             boy riding bike around room girl\n",
            "Name: caption, dtype: object\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "bday = ['young girl sitting in a table talks with plate food'\n",
        ",'some children look into a bag and take out something'\n",
        ",'woman comes out of bathroom downstairs laughing'\n",
        ",'women is driving in car and talking'\n",
        ",'a boy is riding a bike around the room with a girl'\n",
        ",'a girl wearing a bag is standing on the stairs and talking'\n",
        ",'a girl blowing baloon boy pumping up them'\n",
        ",'a girl and boy looks at phone takes it'\n",
        ",'the children play with balloons hitting it'\n",
        ",'two girls sitting in front of cake and put candle on cake'\n",
        ",'girl closes eyes opens blows candle on cake'\n",
        ",'girl and women talk to the camera']\n",
        "\n",
        "bday_df = pd.DataFrame(list(zip(bday)),\n",
        "              columns=['caption'])\n",
        "\n",
        "#stopwords removal\n",
        "bday_df['caption'] = bday_df['caption'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop]))\n",
        "#lemmatization\n",
        "bday_df['caption'] = [''.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]',' ',text)) for text in lis]) for lis in bday_df['caption']]\n",
        "print(bday_df['caption'].head())\n",
        "\n",
        "captions_bday = tokenizer.texts_to_matrix(list(bday_df.caption.values),mode='binary')\n",
        "print(captions_bday[0])\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEqPyNO3B9Y_"
      },
      "source": [
        "## Model Implementations\n",
        ">Models that we have chosen to process futher are:\n",
        "*    Linear Regression\n",
        "*    Support Vector Regression (SVR)\n",
        "*    Random Forest Regression\n",
        "*    Ridge Regression\n",
        "*    Ensemble approach using SVR, Decision Trees, Linear Regression as base estimators and Ridge Regression as final estimator\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j7dsEb_mAMr"
      },
      "source": [
        "### Regression Techniques\n",
        "\n",
        "\n",
        "> As we have seen in the Neural Network section we can choose the feature we need and load it into out 'X' Variabel\n",
        "*    To reduce the code we will be exhibiting the model with best results among the features \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD94YiqUqsQh"
      },
      "source": [
        "#### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "memKeUY1qyIJ"
      },
      "outputs": [],
      "source": [
        "#Short term and long term memorability score \n",
        "Y_LR = labels[['short-term_memorability','long-term_memorability']].values\n",
        "#c3d values\n",
        "X_LR = feature1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR2s4s-3sfwQ"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset into test and train set with 90:10 ratio\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_LR,Y_LR, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_th_Jd2HsftM"
      },
      "outputs": [],
      "source": [
        "# !pip install ​​​scikit-optimize==0.8.1\n",
        "# !pip install scikit-learn==0.22.2\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Creating A Linear Regression Model\n",
        "lr = LinearRegression()\n",
        "\n",
        "#Training the model\n",
        "history = lr.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rp6GHGgYsfqt",
        "outputId": "8c35fbcb-3214-41fe-b465-10f6da327520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600, 2)\n"
          ]
        }
      ],
      "source": [
        "predictionsLR = lr.predict(X_test)\n",
        "print(predictionsLR.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAbjZ5HZuzCy"
      },
      "source": [
        "#### Support Vector Regression -- SVR\n",
        "Finds the best fit line(hyperplane) that has the maximum number of points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2UmD47AzIhC"
      },
      "source": [
        "###### Short Term Memorability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq8jXZo4u1_b"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-73vC8qu18S"
      },
      "outputs": [],
      "source": [
        "Y_short = labels[['short-term_memorability']].values\n",
        "X_SVR = np.concatenate((feature1,feature2,captions), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU58BPpEu15V"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset into test and train set with 90:10 ratio\n",
        "X_train, X_test, Y_train_short, Y_test_short = train_test_split(X_SVR,Y_short, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1Pz0JIDu12Z",
        "outputId": "91df7e40-ff63-4a12-afcb-943090d35223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000 (8000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(len(X_SVR),Y_short.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we6T9B_Nu1m0",
        "outputId": "6bfe87fc-6017-48b2-fbe2-a00de269fc27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "#Creating an SVR model\n",
        "#The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example.\n",
        "modelSVR_short = SVR(C=100).fit(X_train,Y_train_short)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STvmSeoBwIdV"
      },
      "outputs": [],
      "source": [
        "predictionsSVR_short = modelSVR_short.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfGKlS-twIZr",
        "outputId": "667ea2f3-0ff5-45c6-8673-4c846b887f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Spearman's correlation coefficient is: 0.415\n"
          ]
        }
      ],
      "source": [
        "Get_score(predictionsSVR_short,Y_test_short)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1P6_U6qzkHT"
      },
      "source": [
        "###### Long Term Memorability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmaKy08qzvdS"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtLOoL1BzyEr"
      },
      "outputs": [],
      "source": [
        "Y_long = labels[['long-term_memorability']].values\n",
        "X_SVR = np.concatenate((feature1,captions,feature2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdQj6zPezx4n"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset into test and train set with 90:10 ratio\n",
        "X_train, X_test, Y_train_long, Y_test_long = train_test_split(X_SVR,Y_long, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWl6MSgvzx1Q",
        "outputId": "ba781e40-baef-46e3-a675-43a07f1613c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000 (8000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(len(X_SVR),Y_short.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSJtZG-9zxyr",
        "outputId": "9b2cbb3c-695d-460b-8255-adef8d63f6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "modelSVR_long = SVR(C=100).fit(X_train,Y_train_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEGHY4c-zxvk"
      },
      "outputs": [],
      "source": [
        "predictionsSVR_long = modelSVR_long.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR8Dh2tcc4Rd",
        "outputId": "9709735b-9b50-4a7c-d9b0-b7b5bc7f0e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Spearman's correlation coefficient is: 0.170\n"
          ]
        }
      ],
      "source": [
        "Get_score(predictionsSVR_long,Y_test_long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlL-QhdTOrhj"
      },
      "source": [
        "#### Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7cvxqxVEWTx"
      },
      "outputs": [],
      "source": [
        "Y_NN = labels[['short-term_memorability','long-term_memorability']].values\n",
        "X_NN =np.concatenate((captions,feature1,feature2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2hcTfm0HVF3"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset into test and train set with 90:10 ratio\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_NN,Y_NN, test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FZtGOScchTu",
        "outputId": "4d845743-f7d3-4053-cfa8-5f87d66b6154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7200, 11953) (7200, 2)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,Y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK0oU92MO0Ia"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "clf = Ridge()\n",
        "\n",
        "#Creating an Ridge Regression Model\n",
        "clf.set_params(alpha=45)\n",
        "model = clf.fit(X_train, Y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrVkEOhoO3JF"
      },
      "outputs": [],
      "source": [
        "predictionsRR = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8a3ndnVV7c4",
        "outputId": "2b8a25cc-081c-4cb4-bc79-7205afe70f20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "predictionsRR.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjWFRnwTO5V5",
        "outputId": "ff931bd4-2f45-47a3-84e5-1f0701b5b8e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Spearman's correlation coefficient is: 0.454\n",
            "The Spearman's correlation coefficient is: 0.185\n"
          ]
        }
      ],
      "source": [
        "Get_score(predictionsRR, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTByZEJz4W2h"
      },
      "source": [
        "#### Random Forest\n",
        " A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ofaKs734dho"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9mKdo6U4dea"
      },
      "outputs": [],
      "source": [
        "#Short term and long term memorability score \n",
        "Y_RF = labels[['short-term_memorability','long-term_memorability']].values\n",
        "X_RF = np.concatenate((feature1,feature2,captions), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpSIYmmA4dba",
        "outputId": "590a88be-9886-41f8-a2b8-4492ac41574f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 1 of 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   17.8s remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building tree 2 of 20\n",
            "building tree 3 of 20\n",
            "building tree 4 of 20\n",
            "building tree 5 of 20\n",
            "building tree 6 of 20\n",
            "building tree 7 of 20\n",
            "building tree 8 of 20\n",
            "building tree 9 of 20\n",
            "building tree 10 of 20\n",
            "building tree 11 of 20\n",
            "building tree 12 of 20\n",
            "building tree 13 of 20\n",
            "building tree 14 of 20\n",
            "building tree 15 of 20\n",
            "building tree 16 of 20\n",
            "building tree 17 of 20\n",
            "building tree 18 of 20\n",
            "building tree 19 of 20\n",
            "building tree 20 of 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  6.1min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<7200x8186 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 1576481 stored elements in Compressed Sparse Row format>, array([   0,  305,  800, 1467, 1742, 2251, 2694, 3095, 3334, 3755, 4202,\n",
            "       4651, 4910, 5259, 5628, 6063, 6466, 6939, 7470, 7835, 8186]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
          ]
        }
      ],
      "source": [
        "#Splitting dataset into test and train set with 90:10 ratio\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_RF,Y_RF, test_size=0.10, random_state=42)\n",
        "\n",
        "#setting parameters for Random Forest\n",
        "# n_estimators = number of trees max_depth = maximium depth of the tree\n",
        "rf = RandomForestRegressor(n_estimators=20,max_depth=10,random_state=50,verbose=2)\n",
        "\n",
        "#training dataset\n",
        "rf.fit(X_train,Y_train)\n",
        "\n",
        "print(rf.decision_path(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_zpWhsG4dYi",
        "outputId": "dabc8e9b-5b47-49fd-b8f9-986c9acf02e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.0s finished\n"
          ]
        }
      ],
      "source": [
        "predictionsRF = rf.predict(X_test)\n",
        "print(predictionsRF.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-p1a-_54dVU",
        "outputId": "20d6de82-f380-4825-deda-b6ef42e39803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Spearman's correlation coefficient is: 0.223\n",
            "The Spearman's correlation coefficient is: 0.120\n"
          ]
        }
      ],
      "source": [
        "Get_score(predictionsRF,Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vaf5sYrfQ7SL"
      },
      "source": [
        "### Stacked Regression - Ensemble approach\n",
        ">Stacking, an ensemble machine learning algorithm that learns how to best combine each of the models in an ensemble to come up with the best performance.\n",
        "\n",
        ">Level-0 Models (Base-Models): Models fit on the training data and whose predictions are compiled.\n",
        "Level-1 Model (Meta-Model): Model that learns how to best combine the predictions of the base models.\n",
        "\n",
        "> Ensemble approach using\n",
        "SVR, Decision Tree’s and Linear Regression as base\n",
        "classifiers and Ridge Linear Regression as the final\n",
        "estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOMEOBMtiXMR"
      },
      "source": [
        "#### Short Term Memorability Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj6yPALVZzPl"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vElRZvogaltS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#Short term and long term memorability score \n",
        "Y_Stack = labels[['short-term_memorability']].values\n",
        "X_Stack = captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBqzoBdEalp6"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset into test and train set with 90 : 10 ratio\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_Stack,Y_Stack, test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeV075t0XdKI",
        "outputId": "6d6f11d2-0491-407e-d993-f3d6a4509a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/ensemble/_stacking.py:957: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/linear_model/_ridge.py:216: LinAlgWarning: Ill-conditioned matrix (rcond=2.62505e-24): result may not be accurate.\n",
            "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "estimators = [\n",
        "    ('regr', DecisionTreeRegressor(max_depth=10)),\n",
        "    ('svr', SVR(C=100)),\n",
        "    ('Lr',LinearRegression())\n",
        "]\n",
        "reg = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=Ridge(alpha=45)\n",
        ")\n",
        "\n",
        "model = reg.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnxob0mMa7jS"
      },
      "outputs": [],
      "source": [
        "StackPredictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvCIAJX0zk-S",
        "outputId": "553d96f9-fd6c-4f3f-c6dd-cd34064b4525"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYWoageEh0m4",
        "outputId": "870311b3-ba36-4852-d30e-fd62012e9999"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((800, 5777), (800, 1), (800,))"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "X_test.shape,Y_test.shape,StackPredictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTanK6HF-GF-",
        "outputId": "469b85cf-ee87-4cc8-b77f-0a32e4c499b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short_Term Memorability\n",
            "0.8455822336120644\n",
            "0.8616276299006287\n",
            "0.8607597382831513\n",
            "0.8616069728065945\n",
            "0.8609540891754777\n",
            "0.8624627006631178\n",
            "0.8510350233753313\n",
            "0.8613135752630425\n",
            "0.8638120459583756\n",
            "0.853517146105826\n"
          ]
        }
      ],
      "source": [
        "print(\"Short_Term Memorability\")\n",
        "for i in range(10):\n",
        "  print(StackPredictions[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STgPzLrCF-Et",
        "outputId": "6a807694-45a2-4592-8366-1839c063715f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference between the actual and predicited values\n",
            "[0.04941777]\n",
            "[0.05837237]\n",
            "[0.00224026]\n",
            "[0.06339303]\n",
            "[0.05095409]\n",
            "[0.0354627]\n",
            "[0.06503502]\n",
            "[0.11968642]\n",
            "[0.03418795]\n",
            "[0.13351715]\n"
          ]
        }
      ],
      "source": [
        "print(\"Difference between the actual and predicited values\")\n",
        "for i in range(10):\n",
        "  print(abs(StackPredictions[i]-Y_test[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3B4fqyiarPJ",
        "outputId": "e7692d88-cb17-4103-9e66-7303ba9250b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Spearman's correlation coefficient is: 0.440\n"
          ]
        }
      ],
      "source": [
        "Get_score(StackPredictions, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBjfaEKxtgMT",
        "outputId": "8baf6019-5d7f-418a-b6fa-b33522d7b8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 young girl sitting in a table talks with plate food 0.8696345522053622\n",
            "1 some children look into a bag and take out something 0.8586100826193283\n",
            "2 woman comes out of bathroom downstairs laughing 0.8656985789794267\n",
            "3 women is driving in car and talking 0.8628422948175682\n",
            "4 a boy is riding a bike around the room with a girl 0.8672386743630695\n",
            "5 a girl wearing a bag is standing on the stairs and talking 0.8675293532849079\n",
            "6 a girl blowing baloon boy pumping up them 0.8670025074512311\n",
            "7 a girl and boy looks at phone takes it 0.8674027969114809\n",
            "8 the children play with balloons hitting it 0.8556852995215515\n",
            "9 two girls sitting in front of cake and put candle on cake 0.863903695511688\n",
            "10 girl closes eyes opens blows candle on cake 0.8657154137385624\n",
            "11 girl and women talk to the camera 0.8680296160879852\n"
          ]
        }
      ],
      "source": [
        "Predictions_bday = model.predict(captions_bday)\n",
        "for i in range(len(Predictions_bday)):\n",
        "  if(Predictions_bday[i]>0.85):\n",
        "    print(i,bday[i],Predictions_bday[i])\n",
        "  else:\n",
        "    print('low')\n",
        "    print(bday[i],Predictions_bday[i])\n",
        "\n",
        "test_mem_score = pd.DataFrame(list(zip(bday, Predictions_bday)),\n",
        "              columns=['captions','mem_scores'])\n",
        "\n",
        "# test_mem_score.sort_values('mem_scores')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9deb-p1aidxu"
      },
      "source": [
        "#### Long Term Memorability Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VT6zEx2hfMCv"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import StackingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3lXLuuBiiuL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate, cross_val_predict\n",
        "\n",
        "#long term memorability score \n",
        "Y_Stack = labels[['long-term_memorability']].values\n",
        "X_Stack = np.concatenate((captions,feature2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPaXQ-vFinI6"
      },
      "outputs": [],
      "source": [
        "#Splitting dataset into test and train set with 90:10 ratio\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_Stack,Y_Stack, test_size=0.10, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc6CVKypixWm"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "estimators = [\n",
        "    ('regr', DecisionTreeRegressor(max_depth=10)),\n",
        "    ('svr', SVR(C=100)),\n",
        "    ('Lr',LinearRegression())\n",
        "]\n",
        "reg = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=Ridge(alpha=45)\n",
        ")\n",
        "\n",
        "model = reg.fit(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tStOEbuQi4lz"
      },
      "outputs": [],
      "source": [
        "StackPredictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_05ES5eXjIN5"
      },
      "outputs": [],
      "source": [
        "X_test.shape,Y_test.shape,StackPredictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKIB4P-MIrgl"
      },
      "outputs": [],
      "source": [
        "print(\"Long_Term Memorability\")\n",
        "for i in range(10):\n",
        "  print(StackPredictions[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn5fPVk7Ixq6"
      },
      "outputs": [],
      "source": [
        "print(\"Difference between the actual and predicited values\")\n",
        "for i in range(10):\n",
        "  print(abs(StackPredictions[i]-Y_test[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnIkve9KjLLf"
      },
      "outputs": [],
      "source": [
        "Get_score(StackPredictions, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiN-trIxWryM"
      },
      "source": [
        "# References\n",
        "\n",
        "* https://pypi.org/project/PyPrind/\n",
        "* https://en.wikipedia.org/wiki/Keras\n",
        "* https://www.tensorflow.org/\n",
        "* https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient\n",
        "* https://github.com/dazcona/memorability\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}